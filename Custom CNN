# ECE 579 W24 Final Project - Debris Detection
# Python Code used in Google Colab
# Team members: Fazal Mohammed, George Hauck, Julia Korde

# Custom CNN - Hyperparameter Tuning

# Download annotations and images from Roboflow (train, val, test folders)
!pip install roboflow
from roboflow import Roboflow
rf = Roboflow(api_key="REMOVED") #Removed API key
project = rf.workspace("579").project("final-project-debris-detection")
version = project.version(1)
dataset = version.download("tensorflow")

# Set some hyperparameters
Num_of_Epochs = 10 # also used 1, 3, 5, 15
Activation_Func = 'sigmoid' # also used 'hard_sigmoid,' 'relu,' 'linear'
Batch_Size = 16 # also used 8, 32
Learning_Rate = 0.001 # also used 0.01, 0.0001
Momentum = 0 # also used 0.99, 1, 10
Optimizer = 'SGD' # also used 'adam,' 'rmsprop'

# Import libraries
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
from tensorflow.keras.utils import Sequence
from PIL import Image

# Define the class names and the number of classes
class_names = ['road', 'debris']
num_classes = len(class_names)

# Data generator class
class DataGenerator(Sequence):
    def __init__(self, base_path, dataset, class_names, batch_size=8, dim=(320, 320), n_channels=3, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.dataset = dataset
        self.base_path = base_path
        self.n_channels = n_channels
        self.class_names = class_names
        self.n_classes = len(class_names)
        self.shuffle = shuffle
        self.annotations_path = os.path.join(base_path, dataset, '_annotations.csv')
        self.df = pd.read_csv(self.annotations_path)
        self.indexes = np.arange(len(self.df))
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        return self.__data_generation(indexes)

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, indexes):
        'Generates data containing batch_size samples'
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size, 4 + self.n_classes), dtype=np.float32)  # Modify according to output structure

        for i, idx in enumerate(indexes):
            row = self.df.iloc[idx]
            image_path = os.path.join(self.base_path, self.dataset, row['filename'])
            image = Image.open(image_path)
            image = image.resize(self.dim)
            X[i, ] = np.array(image) / 255.0

            bboxes = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]
            classes = pd.get_dummies(row['class'], dtype=np.float32).reindex(columns=self.class_names, fill_value=0).to_numpy()
            y[i, ] = np.hstack([bboxes, classes.flatten()])

        return X, y

#  Set up CNN class
def Net(num_classes, Activation_Function=Activation_Func):
    model = Sequential([
        # First convolutional layer
        Conv2D(16, (3, 3), activation=Activation_Function, input_shape=(320, 320, 3)),
        MaxPooling2D(2, 2),

        # Second convolutional layer
        Conv2D(32, (3, 3), activation=Activation_Function),
        MaxPooling2D(2, 2),

        # Third convolutional layer
        Conv2D(64, (3, 3), activation=Activation_Function),
        MaxPooling2D(2, 2),

        # Fourth convolutional layer
        Conv2D(128, (3, 3), activation=Activation_Function),
        MaxPooling2D(2, 2),

        # Fifth convolutional layer
        Conv2D(256, (3, 3), activation=Activation_Function),
        MaxPooling2D(2, 2),

        #Added and removed Conv2D + MaxPooling2D for experimenting with 2, 6, and 10 layers

        # Flatten the results to feed into a dense layer
        Flatten(),

        # Dense layer for interpretation and classification
        Dense(128, activation=Activation_Function),
        Dropout(0.5),

        # Output layer adjusted according to the number of classes
        Dense(4 + num_classes, activation=Activation_Function)  # Use softmax for multi-class classification
    ])
    return model

base_path = '/content/Final-Project:-Debris-Detection-1'
model1 = Net(num_classes)
model1.compile(optimizer=Optimizer, loss='mse', metrics=['accuracy'])

checkpoint_path = "Model_Checkpoints/model_epoch_{epoch:02d}"
checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_loss', mode='min', verbose=1, save_format='tf')
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
tensorboard = TensorBoard(log_dir="Logs", histogram_freq=1)

train_generator = DataGenerator(base_path, 'train', class_names, batch_size=Batch_Size)
valid_generator = DataGenerator(base_path, 'valid', class_names, batch_size=Batch_Size)
test_generator = DataGenerator(base_path, 'test', class_names, batch_size=Batch_Size)

# Train the model
history = model1.fit(
    train_generator,
    validation_data=valid_generator,
    epochs=Num_of_Epochs,
    callbacks=[checkpoint, early_stopping, tensorboard],
    verbose=1
)

#  Test the model
test_loss, test_accuracy = model1.evaluate(test_generator)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")
